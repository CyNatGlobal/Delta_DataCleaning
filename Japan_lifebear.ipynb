{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ZsIO7h-jUI9lf6HBbZ9bYB56FXfCQPX3",
      "authorship_tag": "ABX9TyMoPNrfRdpMSFJyho0XEoSN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RBCynatGlobal/Delta_DataCleaning/blob/main/Japan_lifebear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv"
      ],
      "metadata": {
        "id": "fck2XfP6O8k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: read csv \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\" print  head and change the seperator to a ';', set to low memory = 'True'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\", sep=';', low_memory=True)\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-vd05KGO_fH",
        "outputId": "f45ce28a-8858-4c74-8e2c-a1fb41935512",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id    login_id              mail_address                          password  \\\n",
            "0   1    sugimoto   sugimoto@lifebear.co.jp  f0bac04aa1b45cf443d722d6f71c0250   \n",
            "1   2         kou  nakanishi@lifebear.co.jp  48207c322ee5bb156ffec9f08c960aaa   \n",
            "2   3      yusuke     yuozawa1208@gmail.com  048261a8024ce51d379eb53cc51aaf33   \n",
            "3   4  entyan1106        endo1106@gmail.com  cd77a9dac26260a104facda5665eb3ab   \n",
            "4   5      kuriki          kuriki@wavy4.com  a026597c294cc48cd20ae361f10cbab1   \n",
            "\n",
            "            created_at          salt birthday_on  gender  \n",
            "0  2012-01-13 22:54:05  yGwBKynnsctI  1984-11-09     0.0  \n",
            "1  2012-01-14 12:48:31  aha6EuRYCDvU  1986-11-13     0.0  \n",
            "2  2012-01-17 15:33:22  PVS59dPWk9BH  1984-12-08     0.0  \n",
            "3  2012-01-17 15:37:02  vLZI6TVCJowN  1987-11-06     0.0  \n",
            "4  2012-01-17 18:52:32  swFznWWk79fg  1986-10-21     0.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gh4_v1d6eDBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate a python script to break a .csv file into smaller files for ingestion\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def split_csv(input_file, output_dir, chunk_size):\n",
        "    \"\"\"\n",
        "    Splits a large CSV file into smaller files.\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to the input CSV file.\n",
        "        output_dir (str): Path to the directory where the output files will be saved.\n",
        "        chunk_size (int): Number of rows per output file.\n",
        "    \"\"\"\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for i, chunk in enumerate(pd.read_csv(input_file, chunksize=chunk_size)):\n",
        "        output_file = os.path.join(output_dir, f\"chunk_{i}.csv\")\n",
        "        chunk.to_csv(output_file, index=False)\n",
        "        print(f\"Created {output_file}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "input_csv = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"  # Replace with your input file\n",
        "output_directory = \"/content/split_csv_files\"  # Replace with your desired output directory\n",
        "rows_per_file = 100000  # Adjust as needed\n",
        "\n",
        "split_csv(input_csv, output_directory, rows_per_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DZ1Se7pWRyp",
        "outputId": "a41a0bb2-8a8c-442e-8126-6180e2f7c10e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created /content/split_csv_files/chunk_0.csv\n",
            "Created /content/split_csv_files/chunk_1.csv\n",
            "Created /content/split_csv_files/chunk_2.csv\n",
            "Created /content/split_csv_files/chunk_3.csv\n",
            "Created /content/split_csv_files/chunk_4.csv\n",
            "Created /content/split_csv_files/chunk_5.csv\n",
            "Created /content/split_csv_files/chunk_6.csv\n",
            "Created /content/split_csv_files/chunk_7.csv\n",
            "Created /content/split_csv_files/chunk_8.csv\n",
            "Created /content/split_csv_files/chunk_9.csv\n",
            "Created /content/split_csv_files/chunk_10.csv\n",
            "Created /content/split_csv_files/chunk_11.csv\n",
            "Created /content/split_csv_files/chunk_12.csv\n",
            "Created /content/split_csv_files/chunk_13.csv\n",
            "Created /content/split_csv_files/chunk_14.csv\n",
            "Created /content/split_csv_files/chunk_15.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EHdVQASc20Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"/content/split_csv_files/chunk_15.csv\", sep=';', low_memory=True) # removed the extra space at the end of the filename\n",
        "print(df.head())"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7hP1Bh_22N9",
        "outputId": "031c2668-1789-431b-c62c-86170b4ed7ab"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        id     login_id                     mail_address  \\\n",
            "0  3195453      chalten         losglaciares@hotmail.com   \n",
            "1  3195460    ryo630617            rr.rr.rr@docomo.ne.jp   \n",
            "2  3195470   yusaku0809  yusaku_tokunaga@r.recruit.co.jp   \n",
            "3  3195479       shuri7               shuri.jb@gmail.com   \n",
            "4  3195488  iketata3333          iketata3333@ezweb.ne.jp   \n",
            "\n",
            "                           password           created_at          salt  \\\n",
            "0  c1e409295fb048d67638712dc2d6cdd1  2014-10-29 00:58:41  iDAPZLmX2D3s   \n",
            "1  852fffe0d102e27e60deb1c767f934c3  2014-10-29 00:59:43  T85NbOgZzqbQ   \n",
            "2  a9caa4c66322e6b42a1cbb9497703ff5  2014-10-29 01:01:03  f5zhLEPbwlgP   \n",
            "3  5ca8f4b1691bebbfea10209b6a7418a2  2014-10-29 01:02:25  qYgjxslrNRkV   \n",
            "4  97a2f8d95f829a255b4a4e2d98229afc  2014-10-29 01:03:26  ycK9oMHN0Xec   \n",
            "\n",
            "   birthday_on  gender  \n",
            "0          NaN     NaN  \n",
            "1          NaN     NaN  \n",
            "2          NaN     NaN  \n",
            "3          NaN     NaN  \n",
            "4          NaN     0.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FfmiI7ciInQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a dataframe where all data is put under their correct headers and create a junk file for unused values or unfinished data\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming you have a DataFrame named 'df' with some data and headers.\n",
        "# You can replace 'df' with the actual DataFrame you are working with.\n",
        "\n",
        "# Create a dictionary to map the correct column names to the existing columns\n",
        "header_mapping = {\n",
        "    'id': 'id',\n",
        "    'login_id': 'login_id',\n",
        "    'mail_address': 'mail_address',\n",
        "    'password': 'password',\n",
        "    'created_at': 'created_at', # Enclose 'created_at' in quotes\n",
        "    'salt': 'salt', # Enclose 'salt' in quotes\n",
        "    'birthday_on': 'birthday_on',\n",
        "    'gender': 'gender',\n",
        "    # ... add more mappings for your columns\n",
        "}\n",
        "\n",
        "# Create a new DataFrame with the correct header names\n",
        "new_df = pd.DataFrame()\n",
        "for new_header, old_header in header_mapping.items():\n",
        "  if old_header in df.columns:\n",
        "    new_df[new_header] = df[old_header]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Save the new DataFrame (with correct headers) to a new CSV file\n",
        "new_df.to_csv('cleaned_data_15.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Cleaned DataFrame saved to cleaned_data_0.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_805LGdxP4k",
        "outputId": "bf128d7c-07a7-42af-a9be-71d3d31d2d9b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned DataFrame saved to cleaned_data_0.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: using( /content/split_csv_files/chunk_0.csv. )data ,create a garbage_data_csv dataframe where invalid and duplicate data will be placed and give me logging information of how much data had been placed in the garbage_data_csv\n",
        "\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import logging\n",
        "import re\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(filename='garbage_data_log.txt', level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "df = pd.read_csv(\"/content/split_csv_files/chunk_15.csv\", sep=';', low_memory=False)\n",
        "\n",
        "# Create an empty DataFrame for garbage data\n",
        "garbage_data_csv = pd.DataFrame()\n",
        "\n",
        "# Example: Identify invalid data (e.g., rows with missing 'id')\n",
        "invalid_rows = df[df['id'].isnull()]\n",
        "if not invalid_rows.empty:\n",
        "  garbage_data_csv = pd.concat([garbage_data_csv, invalid_rows])\n",
        "  logging.info(f\"{len(invalid_rows)} rows with missing 'id' added to garbage_data_csv\")\n",
        "\n",
        "# Function to validate email\n",
        "def is_valid_email(email):\n",
        "    \"\"\"Validates the format of a mail address.\"\"\"\n",
        "    pattern = r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$'\n",
        "    return bool(re.match(pattern, email))\n",
        "\n",
        "# Function to check if birthday is a valid date\n",
        "def is_valid_birthday(birthday):\n",
        "    \"\"\"Check if the birthday is a valid date.\"\"\"\n",
        "    try:\n",
        "        pd.to_datetime(birthday, errors='raise')  # Try to convert to datetime\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "# Function to check for missing critical columns\n",
        "def is_valid_row(row):\n",
        "    \"\"\"Validates a row to ensure no critical columns are missing.\"\"\"\n",
        "    if pd.isna(row['login_id']) or pd.isna(row['mail_address']):\n",
        "        return False  # Consider it garbage if login_id or mail_address is missing\n",
        "    return True\n",
        "\n",
        "\n",
        "\n",
        "# Example: Identify duplicate data (e.g., based on 'id')\n",
        "duplicate_rows = df[df.duplicated(subset=['id'], keep=False)]\n",
        "if not duplicate_rows.empty:\n",
        "  garbage_data_csv = pd.concat([garbage_data_csv, duplicate_rows])\n",
        "  logging.info(f\"{len(duplicate_rows)} duplicate rows based on 'id' added to garbage_data_csv\")\n",
        "\n",
        "# Add more checks for invalid and duplicate data as needed\n",
        "\n",
        "# Save the garbage data to a CSV file\n",
        "garbage_data_csv.to_csv('garbage_data.csv', index=False)\n",
        "\n",
        "logging.info(f\"Total of {len(garbage_data_csv)} rows placed in garbage_data_csv\")\n",
        "\n",
        "print(\"Garbage data saved to garbage_data.csv\")\n"
      ],
      "metadata": {
        "id": "QvAfzHYe_4bv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37de2362-2b4e-459c-f0eb-0294f8602b79"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Garbage data saved to garbage_data.csv\n"
          ]
        }
      ]
    }
  ]
}